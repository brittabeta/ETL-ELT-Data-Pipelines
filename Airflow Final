from airflow import DAG 
from airflow.operators.bash_operator import BashOperator
from datetime import timedelta
from airflow.utils.dates import days_ago


# DAG arguments
default_args = {
    'Owner': 'brittabeta',
    'Start_date': days_ago(0), # today
    'email': ['brittabeta@dummy.com'],
    'email_on_failure': True,
    'email_on_retry': True,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}


# DAG definition (instantiation)
dag = DAG ('ETL_toll_data',
description='Apache Airflow Final Assignment',
default_args=default_args,
schedule_interval=timedelta(days=1), # every day
)


# Task definitions
task1 = BashOperator(
    task_id='unzip_data',
    bash_command='tar -xzf /home/project/tolldata.tgz',
    dag=dag,
)

task2 = BashOperator(
    task_id='extract_data_from_csv',
    bash_command='cut f1,4 -d"," vehicle-data.csv > csv_data.csv',
    dag=dag,
)

task3 = BashOperator(
    task_id='extract_data_from_tsv',
    bash_command='cut -f5,7 -d"/t" tollplaza-data.tsv > tsv_data.csv',
    dag=dag,
)

task4 = BashOperator(
    task_id='extract_data_from_fixed_width',
    bash_command='awk 'NF{print $(NF-1),$NF} OFS="\t" payment-data.txt > fixed_width_data.csv',
    dag=dag,
)

task5 = BashOperator(
    task_id='consolidate_data',
    bash_command='paste csv_data.csv tsv_data.csv fixed_width_data.csv > extracted_data.csv',
    dag=dag,
)

task6 = BashOperator(
    task_id='transform_data',
    bash_command='awk '$5 = toupper($5)' < extracted_data.csv > transformed_data.csv',
    dag=dag,
)
# Task Pipeline
task1 >> task2 >> task3 >> task4 >> task5 >> task6
